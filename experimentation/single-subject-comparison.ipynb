{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf522c7e97400249",
   "metadata": {},
   "source": [
    "# Multi-Target Regression\n",
    "\n",
    "This notebook was made as part of a project for the course \"Statistical Foundations of Machine Learning\".\n",
    "It has been included in this repository because it illustrates part of the experimentation phase of our research.\n",
    "Of course, this notebook displays a very isolated and down-scaled version of the problem.\n",
    "In reality, things were a lot more complex and not as perfectly structured as in this notebook.\n",
    "Regardless, it gives some insight into how conclusions were drawn and substantiates the claim that we did, in fact, compare all approaches discussed in the methodology.\n",
    "In particular, you will find a simple implementation of the stacked regressor, the results of which were not included in the thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9040d5d1926487a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T08:50:51.914769Z",
     "start_time": "2024-06-02T08:50:45.164235Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4f18aa897d591aff",
   "metadata": {},
   "source": [
    "# 1 - Problem Understanding\n",
    "\n",
    "The research question covered in this notebook is inspired by Arno's master's thesis.\n",
    "The aim of that thesis is to estimate ground reaction forces (**GRFs**), i.e. the forces exerted by the ground on a person's feet.\n",
    "These estimations have to be based on measurements made by smart insoles (Moticon OpenGo) that can only measure pressure and inertia (at 2 ms intervals).\n",
    "\n",
    "## 1.1 - The Dataset\n",
    "\n",
    "To collect data, Arno had to wear the insoles and walk over force plates that accurately measure GRFs.\n",
    "That's how `gait_cycles.csv` came about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T09:51:21.822851Z",
     "start_time": "2024-05-20T09:51:19.849061Z"
    }
   },
   "source": [
    "gait_cycles = pd.read_csv('gait_cycles.csv')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "96a49fccdd628768",
   "metadata": {},
   "source": [
    "Each row in this dataset contains the \n",
    "- GRF of the left foot at timestamp $t$;\n",
    "- GRF of the right foot at timestamp $t$.\n",
    "\n",
    "These are the **target labels $Y$** of the ML problem.\n",
    "Since we consider the world around us to be three-dimensional, these forces are measured along an $x$-, $y$- and $z$-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f7e7cc27420fd14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T09:51:21.854109Z",
     "start_time": "2024-05-20T09:51:21.822851Z"
    }
   },
   "source": [
    "LABELS = ['Fx_l', 'Fy_l', 'Fz_l', # GRF of left foot\n",
    "          'Fx_r', 'Fy_r', 'Fz_r'] # GRF of right foot\n",
    "\n",
    "Y = gait_cycles[LABELS]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d2608086fd3c51c4",
   "metadata": {},
   "source": [
    "Additionnally, each row contains the insole and sEMG signals\n",
    "- at timestamp $t$\n",
    "- at 6 previous timestamps (from $t - 2\\mathrm{ms}$ until $t - 64\\mathrm{ms}$)\n",
    "- at 6 future timestamps (from $t + 2\\mathrm{ms}$ until $t + 64\\mathrm{ms}$)\n",
    "\n",
    "These are the **input features $X$** of the ML problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a3d319a8e6ee596",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T09:51:21.907527Z",
     "start_time": "2024-05-20T09:51:21.854109Z"
    }
   },
   "source": [
    "def extract_features(df):\n",
    "    X = df.drop(columns=LABELS, axis=1)      # drop labels\n",
    "    return X.drop(columns=['trial'], axis=1) # drop trial \n",
    "\n",
    "X = extract_features(gait_cycles)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4093b272a33034bf",
   "metadata": {},
   "source": [
    "Finally, each row mentions the trial/choreagraphy in which this datapoint was measured.\n",
    "Some trials were pretty simple (like \"standing\" or \"forward walking\"), while others were more complex (like \"circular jogging\").\n",
    "We'll refer to this column as the **strata**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1348b32d9a30f521",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T09:51:21.923151Z",
     "start_time": "2024-05-20T09:51:21.907527Z"
    }
   },
   "source": [
    "strata = gait_cycles['trial']"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ca6ccaaa66b3fc0c",
   "metadata": {},
   "source": [
    "## 1.2 - The Model\n",
    "\n",
    "The aim is to train an artificial neural network (ANN) that outputs GRF estimations for both feet:\n",
    "$ \\vec{y} = \n",
    "    \\begin{bmatrix}\n",
    "        F{x_l} \\\\\n",
    "        F{y_l} \\\\\n",
    "        F{z_l} \\\\\n",
    "        F{x_r} \\\\\n",
    "        F{y_r} \\\\\n",
    "        F{z_r}\n",
    "    \\end{bmatrix}\n",
    "$.\n",
    "This means we are dealing with a **multi-target regression model**, which gave rise to the research question discussed in this notebook.\n",
    "\n",
    "With regard to the input of the model, we consider two approaches. In both cases, input vector $\\vec{x}$ originates from the insole measurements of both feet (i.e. rows of $X$ in `gait_cycles.csv`).\n",
    "1. In the first approach, these measurements will directly serve as input features of the model.\n",
    "2. In the second approach the measurements will first be projected onto a lower dimensional space with PCA.\n",
    "\n",
    "The following diagram gives a high-level overview of this process.\n",
    "\n",
    "![A high-level diagram of the ML-problem at hand](images/problem_understanding.png)\n",
    "\n",
    "#### Note\n",
    "We've implemented the various versions of this model with the use of PyTorch.\n",
    "This Python library makes use of a datastructure called **Tensor**.\n",
    "For this reason we have to make conversions from NumPy arrays or Pandas DataFrames to Tensors, or vice-versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74c3f5c9877d4845",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T09:51:21.970020Z",
     "start_time": "2024-05-20T09:51:21.923151Z"
    }
   },
   "source": [
    "X = torch.tensor(X.values, dtype=torch.float32)\n",
    "Y = torch.tensor(Y.to_numpy().reshape((-1, 6)), dtype=torch.float32)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e4bf7cf9350943fe",
   "metadata": {},
   "source": [
    "# 2 - Research Question\n",
    "\n",
    "The common thread among the following experiments is the following question:\n",
    "\n",
    "**What's the optimal approach for leveraging ANNs to tackle multi-target regression problems?**.\n",
    "\n",
    "Although this fully captures the essence of this chapter, it should be noted that this formulation is not thorough enough to serve as a guiding research question.\n",
    "It is imprecise for several reasons:\n",
    "- First, it does not specify what can be considered as an \"**approach**\" or how it relates to the model. We have chosen to include PCA and multi-target methods.\n",
    "- Second, it's unclear what is meant by \"**optimal**\". According to which metric will approaches be compared? We have chosen for out-of-sample error and will limit our scope to this particular dataset.\n",
    "- Finally, what does it mean for a multi-target regression problem to be \"**tackled**\"? Special attention should be paid to the relationship between target variables.\n",
    "\n",
    "For these reasons, the research question was split up into two sub-questions, each focussing on a seperate approach:\n",
    "\n",
    "1. What's the impact of **PCA** on the network's in- and out-of-sample error?\n",
    "\n",
    "2. How do different **multi-target regression methods** compare in terms of in- and out-of-sample error?\n",
    "\n",
    "   We will consider:\n",
    "    - single-target method\n",
    "    - regressor Stacking\n",
    "    - multi-task learning with hard parameter sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac311220c87f5d6f",
   "metadata": {},
   "source": [
    "# 3 - Training/Validation/Test Split Strategy\n",
    "\n",
    "We opted for a **stratified** 80/20 train/test-split.\n",
    "With `gait_cycles['trials']` as the strata, the same proportion samples will be split for each gait_cycle.\n",
    "We want the trained model to perform well on all types of gait trials.\n",
    "This strategy ensures that the test set contains the necessary data to make that evaluation.\n",
    "In an extreme counter scenario: a purely random split could result in a test set that doesn't include any data on \"circular jogging\".\n",
    "Error measures computed on that test set would therefore not be representative for that gait trial.\n",
    "\n",
    "The following image illustrates this strategy. \n",
    "For illustration purposes, the rightmost portion of each have trial has been shaded as test set.\n",
    "Note, however, that these 80/20 splits **within** each stratum are **random**.\n",
    "\n",
    "![Train-test-split](images/train-test-split.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db0d4d924fdfe7d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T09:51:22.855742Z",
     "start_time": "2024-05-20T09:51:21.970020Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Make the same stratified split for X, Y and strata\n",
    "(X_full_train,      X_test, \n",
    " Y_full_train,      Y_test, \n",
    " strata_full_train, strata_test) = train_test_split(X, Y, strata, test_size=0.2, random_state=42, stratify=strata)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9083c78bb533be16",
   "metadata": {},
   "source": [
    "The same strategy will be followed for the train-validation split.\n",
    "The full training set will be used for **cross-validation** over 5 stratified folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f80fb9fa658ea79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T09:51:22.871369Z",
     "start_time": "2024-05-20T09:51:22.855742Z"
    }
   },
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Prepare 5 stratified folds for cross-validation\n",
    "K = 5\n",
    "kf = StratifiedKFold(n_splits=K, shuffle=True, random_state=42)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12945394bad0e356",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T14:39:33.087406Z",
     "start_time": "2024-05-20T14:39:32.001330Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def perform_pca(X_train, X_test):\n",
    "    # Normalize features so their variances are comparable\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled  = scaler.transform(X_test)\n",
    "    \n",
    "    # Fit PCA model to the training data that captures 99% of the variance\n",
    "    pca = PCA(n_components=0.99, svd_solver='full')\n",
    "    pca.fit(X_train_scaled)\n",
    "\n",
    "    # Project data from old features to principal components\n",
    "    X_pc_train = pca.transform(X_train_scaled)\n",
    "    X_pc_test  = pca.transform(X_test_scaled)\n",
    "    \n",
    "    # Convert the result to tensor\n",
    "    X_train_tensor = torch.tensor(X_pc_train, dtype=torch.float32)\n",
    "    X_test_tensor  = torch.tensor(X_pc_test,  dtype=torch.float32)\n",
    "\n",
    "    return X_train_tensor, X_test_tensor\n",
    "\n",
    "X_pc_full_train, X_pc_test = perform_pca(X_full_train, X_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2c63ee7119e5a792",
   "metadata": {},
   "source": [
    "Later on, we will have 2 versions for each of our multi-target regression methods.\n",
    "In the first one, we'll use more of an end-to-end approach, and skip this PCA-function altogether.\n",
    "In the second version, we will call this function on the relevant datasets before training the models."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4 - Principal Component Analysis\n",
    "\n",
    "PCA is a form of dimensionality reduction used to simplify the input data of a model.\n",
    "Its aim is to express the data with a new, smaller set of abstract variables, in a way that preserves as much of the original information as possible.\n",
    "It does so by transforming each datapoint to a lower dimensional space.\n",
    "The following illustration, found in \"An Introduction to Statistical Learning\" by James G., Witten D. , Hastie T., Tibshirani R. and Taylot J., serves as a handy visualization.\n",
    "\n",
    "![](images/pca.png)\n",
    "\n",
    "## 4.1 - Computation\n",
    "\n",
    "*The following explanation was inspired by a book of Brunton & Kutz titled \"Data Driven Science & Engineering - Machine Learning, Dynamical Systems, and Control\".*\n",
    "\n",
    "Let's consider an input set of $N$ training samples, each of which is a $d$-dimensional data point:\n",
    "$$\n",
    "X_{\\mathrm{train}} =\n",
    "    \\begin{bmatrix}\n",
    "        \\vec{x}^\\intercal_{0} \\\\\n",
    "        \\vec{x}^\\intercal_{1} \\\\\n",
    "        \\vdots \\\\\n",
    "        \\vec{x}^\\intercal_{N-1}\n",
    "    \\end{bmatrix}\n",
    "\\in \\mathbb{R}^{N \\times d}\n",
    "$$\n",
    "\n",
    "With this notation, we can think of each row as an observation and of each column as a variable.\n",
    "We now want to construct a covariance matrix $C \\in \\mathbb{R}^{d \\times d}$ that lists the covariance between any pair of these variables.\n",
    "It is computed by:\n",
    "1. Calculating the mean of all datapoints\n",
    "    $\\vec{\\mu} = \\frac{1}{N} \\sum_{i = 0}^{N-1} {\\vec{x}_i}$\n",
    "2. Subtracting this mean from every sample in $X$:\n",
    "    $$\n",
    "    B :=\n",
    "        \\begin{bmatrix}\n",
    "            \\vec{x}^\\intercal_{0} - \\vec{\\mu}^\\intercal\\\\\n",
    "            \\vec{x}^\\intercal_{1} - \\vec{\\mu}^\\intercal\\\\\n",
    "            \\vdots \\\\\n",
    "            \\vec{x}^\\intercal_{N-1} - \\vec{\\mu}^\\intercal\n",
    "        \\end{bmatrix}\n",
    "    \\in \\mathbb{R}^{N \\times d}\n",
    "    $$\n",
    "3. Computing the matrix product $C := \\frac{1}{n - 1} B^\\intercal B$.\n",
    "\n",
    "The principal components of $X_{\\mathrm{train}}$ are now defined as the eigenvectors of $C$, for these reveal the directions of maximum variance in the data.\n",
    "We can find them by computing the eigen-decomposition $C = P D P^\\intercal$\n",
    "\n",
    "with\n",
    "$\n",
    "P =\n",
    "\\begin{bmatrix}\n",
    "    \\vec{x}_{\\lambda_0} & \\vec{x}_{\\lambda_1} & ... & \\vec{x}_{\\lambda_{d-1}}\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{d \\times d}\n",
    "$\n",
    "and\n",
    "$\n",
    "D =\n",
    "\\begin{bmatrix}\n",
    "    \\lambda_0 & 0        & ... & 0     \\\\\n",
    "    0         &\\lambda_1 & ... & 0     \\\\\n",
    "    \\vdots    & \\vdots   & \\ddots & \\vdots\\\\\n",
    "    0         & 0        & ... & \\lambda_{d-1}\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{d \\times d}\n",
    "$.\n",
    "\n",
    "The eigenvalues $\\lambda_{0}$, $\\lambda_{1}$, ..., $\\lambda_{d-1}$ in $D$ can be sorted largest to smallest.\n",
    "By sorting their respective eigenvectors $\\vec{x}_{\\lambda_0}$, $\\vec{x}_{\\lambda_{1}}$, ..., $\\vec{x}_{\\lambda_{d-1}}$ in $P$ accordingly, we order them in terms of their contribution to the variance in the training set.\n",
    "Let's say the first $n$ of them cover 99% of it.\n",
    "By removing the last $d - n$ from $P$, we get a new matrix $P' \\in \\mathbb{R}^{d \\times n}$ of $n$ principal components.\n",
    "\n",
    "By multiplying the original training set $X_{\\mathrm{train}}$ by $P'$ we transform it to a lower, $n$-dimensional space, while losing no more than 1% of variance.\n",
    "This transformation is \"stored\" in $P'$ (i.e. it represents a matrix transformation), meaning it can be performed on other datasets, like the test set, as well.\n",
    "\n",
    "## 4.2 - Implementation\n",
    "\n",
    "To investigate the influence of PCA on the models' complexity and performance, we implemented the following function, using scikit-learn.\n",
    "Before PCA, features are normalised so their variances can be compared in a \"fair\" way.\n",
    "To avoid a form of data leakage, the PCA-model is fitted to the training set only.\n",
    "Both training and test set are then transformed according to the same resulting transformation matrix."
   ],
   "id": "a5dca6c5ce475cd3"
  },
  {
   "cell_type": "markdown",
   "id": "ff63af793bdb3499",
   "metadata": {},
   "source": [
    "# 5 - Multilayer Perceptron\n",
    "\n",
    "As seen during the lectures, the multilayer perceptron is a type of feedforward neural network.\n",
    "It's layers are fully connected, meaning each artifical neuron delivers its output to every neuron in the following layer.\n",
    "In the following subsections, we will briefly restate the basic structure of the artificial neuron, as well as the MLP's learning algorithm: gradient descent.\n",
    "\n",
    "\n",
    "## 5.1 - Artificial Neuron\n",
    "Let there be $n \\in \\mathbb{N}_0$ input values:\n",
    "$x_1, x_2, \\dots, x_n \\in \\mathbb{R}.$\n",
    "\n",
    "The artificial neuron computes a linear combination by multiplying each input $x_i$ with a weight $w_i \\in \\mathbb{R}$ and adding a bias term $b \\in \\mathbb{R}$.\n",
    "We can write this bias as the product of an additional constant input $x_0 = 1$ with variable coefficient $w_0$, to allow for more elegant notation:\n",
    "$$\n",
    "\\overbrace{w_0 x_0}^{b} + w_1 x_1 + w_2 x_2 + \\dots + w_n x_n = \\vec{w}^\\intercal \\vec{x} \\in \\mathbb{R}.\n",
    "$$\n",
    "with\n",
    "$ \\vec{w}^\\intercal = \\begin{bmatrix}\n",
    "        w_{0} & w_{1} & \\dots & w_{n}\n",
    "    \\end{bmatrix}\n",
    "$\n",
    "and\n",
    "$ \\vec{x} =\n",
    "    \\begin{bmatrix}\n",
    "        x_{0} \\\\\n",
    "        x_{1} \\\\\n",
    "        \\vdots \\\\\n",
    "        x_{n}\n",
    "    \\end{bmatrix}\n",
    "$.\n",
    "\n",
    "This value is then passed through an nonlinear activation function $\\sigma$ to produce a single output:\n",
    "$\\hat{y} = \\sigma \\left(  \\vec{w}^\\intercal \\vec{x} \\right)$.\n",
    "\n",
    "In our implementation, we chose for a sigmoid activation, namely the logistic function:\n",
    "$\\sigma(x) = logistic(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "\n",
    "## 5.2 - Gradient Descent\n",
    "\n",
    "Gradient descent starts by initializing the MLP's parameters with some random values $\\vec{w}^{(0)} \\in \\mathbb{R}^{n+1}$.\n",
    "\n",
    "The optimal values for $\\vec{w}$ will be found by repeatedly executing the following update rule until convergence:\n",
    "\n",
    "For $i = 0, 1, 2, ...$ compute:\n",
    "$$\\vec{w}^{(i+1)} := \\vec{w}^{(i)} - \\eta \\nabla E(\\vec{w}^{(i)}).$$\n",
    "\n",
    "where $- \\nabla E \\in \\mathbb{R}^{n+1}$ is the direction opposite to the gradient of the cost function and $\\eta \\in \\mathbb{R}$ is the learning rate."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 5 - Multilayer Perceptron\n",
    "\n",
    "As seen during the lectures, the multilayer perceptron is a type of feedforward neural network.\n",
    "It's layers are fully connected, meaning each artifical neuron delivers its output to every neuron in the following layer.\n",
    "In the following subsections, we will briefly restate the basic structure of the artificial neuron, as well as the MLP's learning algorithm: gradient descent.\n",
    "\n",
    "\n",
    "## 5.1 - Artificial Neuron\n",
    "Let there be $n \\in \\mathbb{N}_0$ input values:\n",
    "$x_1, x_2, \\dots, x_n \\in \\mathbb{R}.$\n",
    "\n",
    "The artificial neuron computes a linear combination by multiplying each input $x_i$ with a weight $w_i \\in \\mathbb{R}$ and adding a bias term $b \\in \\mathbb{R}$.\n",
    "We can write this bias as the product of an additional constant input $x_0 = 1$ with variable coefficient $w_0$, to allow for more elegant notation:\n",
    "$$\n",
    "\\overbrace{w_0 x_0}^{b} + w_1 x_1 + w_2 x_2 + \\dots + w_n x_n = \\vec{w}^\\intercal \\vec{x} \\in \\mathbb{R}.\n",
    "$$\n",
    "with\n",
    "$ \\vec{w}^\\intercal = \\begin{bmatrix}\n",
    "        w_{0} & w_{1} & \\dots & w_{n}\n",
    "    \\end{bmatrix}\n",
    "$\n",
    "and\n",
    "$ \\vec{x} =\n",
    "    \\begin{bmatrix}\n",
    "        x_{0} \\\\\n",
    "        x_{1} \\\\\n",
    "        \\vdots \\\\\n",
    "        x_{n}\n",
    "    \\end{bmatrix}\n",
    "$.\n",
    "\n",
    "This value is then passed through an nonlinear activation function $\\sigma$ to produce a single output:\n",
    "$\\hat{y} = \\sigma \\left(  \\vec{w}^\\intercal \\vec{x} \\right)$.\n",
    "\n",
    "In our implementation, we chose for a sigmoid activation, namely the logistic function:\n",
    "$\\sigma(x) = logistic(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "\n",
    "## 5.2 - Gradient Descent\n",
    "\n",
    "Gradient descent starts by initializing the MLP's parameters with some random values $\\vec{w}^{(0)} \\in \\mathbb{R}^{n+1}$.\n",
    "\n",
    "The optimal values for $\\vec{w}$ will be found by repeatedly executing the following update rule until convergence:\n",
    "\n",
    "For $i = 0, 1, 2, ...$ compute:\n",
    "$$\\vec{w}^{(i+1)} := \\vec{w}^{(i)} - \\eta \\nabla E(\\vec{w}^{(i)}).$$\n",
    "\n",
    "where $- \\nabla E \\in \\mathbb{R}^{n+1}$ is the direction opposite to the gradient of the cost function and $\\eta \\in \\mathbb{R}$ is the learning rate."
   ],
   "id": "cde71c877e4f01f9"
  },
  {
   "cell_type": "markdown",
   "id": "5b365c77dc69bbd1",
   "metadata": {},
   "source": [
    "## 5.3 - Implementation\n",
    "\n",
    "We've made a generic implementation for the MLP using PyTorch.\n",
    "It should be instantiated with a list of integers, denoting the number of neurons for any amount of hidden layers.\n",
    "For example: `MLP([16, 32])`creates an MLP with 2 hidden layers. The first has 16 neurons, and the second has 32.\n",
    "The in- and output layers are tailored to the number of features and target outputs of the training set."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 10,
   "source": [
    "from torch import nn\n",
    "from torch.nn import Linear, Sigmoid, MSELoss\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Generic implementation of a multiplayer perceptron (MLP) with sigmoid activation functions\n",
    "\n",
    "    Args:\n",
    "        hidden_sizes (list): list of #neurons in the hidden layer(s).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_sizes):\n",
    "        super().__init__()\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "\n",
    "        # Instantiate hidden layers with sigmoid activations\n",
    "        hidden_layers = [Sigmoid()]\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            hidden_layers.append(Linear(hidden_sizes[i], hidden_sizes[i + 1]))\n",
    "            hidden_layers.append(Sigmoid())\n",
    "\n",
    "        self.__hidden_stack = nn.Sequential(*hidden_layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''Passes input features forward through the MLP's layers\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): input features\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: output of the MLP\n",
    "        '''\n",
    "        X = self.__input_stack(X)\n",
    "        X = self.__hidden_stack(X)\n",
    "        X = self.__output_stack(X)\n",
    "        return X\n",
    "\n",
    "    def train_(self, X, Y):\n",
    "        '''Trains the MLP on the training set [X, Y] through gradient descent\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): input features of the training set\n",
    "            Y (torch.Tensor): target labels of the training set\n",
    "        '''\n",
    "        # Tailor in- and output layer to shape of X and Y\n",
    "        self.__input_stack  = Linear(X.shape[1], self.hidden_sizes[0])\n",
    "        self.__output_stack = Linear(self.hidden_sizes[-1], Y.shape[1])\n",
    "\n",
    "        self.train() # train mode\n",
    "\n",
    "        # Instantiate the optimizer\n",
    "        optimizer = torch.optim.Adam(params=self.parameters(), lr=0.01)\n",
    "\n",
    "        # Instantiate the loss function\n",
    "        loss_function = MSELoss()\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(100):\n",
    "            optimizer.zero_grad() # don't accumulate gradients\n",
    "\n",
    "            # Compute the loss and its gradient\n",
    "            Y_pred = self(X)\n",
    "            loss = loss_function(Y, Y_pred)\n",
    "            loss.backward()  # back propagation\n",
    "\n",
    "            optimizer.step() # adjust weights and biases accordingly\n",
    "\n",
    "\n",
    "    def test(self, X, Y):\n",
    "        '''Computes the MSE of predictions on the test set\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): input features of the test set\n",
    "            Y (torch.Tensor): target labels of the test set\n",
    "\n",
    "        Returns:\n",
    "            float: MSE between predicted and target labels\n",
    "        '''\n",
    "        self.eval() # evaluation mode\n",
    "\n",
    "        # Compare Y to predictions on X\n",
    "        Y_pred = self(X)\n",
    "        loss_function = MSELoss()\n",
    "        mse = loss_function(Y, Y_pred).item()\n",
    "\n",
    "        return mse"
   ],
   "id": "ab59a4a31214a0bb"
  },
  {
   "cell_type": "markdown",
   "id": "318c74cbfeaadfb9",
   "metadata": {},
   "source": [
    "## 5.4 - Training the Model\n",
    "\n",
    "In the following section we will train various models, each making use of the MLP in some way or another.\n",
    "With the `train_` and `test` methods implemented above, we can easily train and score a model on a certain train-validation split.\n",
    "This split is made 5-fold in the following `cross_validate` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37777769cbccd718",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T11:10:31.145891Z",
     "start_time": "2024-05-20T11:10:31.130271Z"
    }
   },
   "source": [
    "def cross_validate(model, X, Y, strata, cv, pca=False):\n",
    "    scores = []\n",
    "    \n",
    "    fold = 0\n",
    "    for train_idx, val_idx in cv.split(X, strata):\n",
    "        fold += 1\n",
    "\n",
    "        # Make train-test split\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        Y_train, Y_val = Y[train_idx], Y[val_idx]\n",
    "        \n",
    "        # Perform PCA if necessary\n",
    "        if pca: X_train, X_val = perform_pca(X_train, X_val)\n",
    "\n",
    "        # Train the model\n",
    "        model.train_(X_train, Y_train)\n",
    "\n",
    "        # Test the model\n",
    "        mse = model.test(X_val, Y_val)\n",
    "        scores.append(mse)\n",
    "\n",
    "    return scores"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "48c14d87d1116bb8",
   "metadata": {},
   "source": [
    "We tuned the following **hyperparameters**, using Optuna:\n",
    "- the number of hidden layers\n",
    "- the number of neurons in each hidden layer\n",
    "\n",
    "The following piece of code gives an example of how this was done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ffc1049-15a5-4b5e-aa98-1b0a7a31927b",
   "metadata": {},
   "source": [
    "import optuna\n",
    "\n",
    "# 1. Define an objective function to be maximized\n",
    "def objective(trial):\n",
    "\n",
    "    # 2. Suggest values for the hyperparameters using a trial object\n",
    "    ## a. Number of layers\n",
    "    n_layers = trial.suggest_int('hidden_layers', 1, 2)\n",
    "    \n",
    "    ## b. Number of neurons per layer\n",
    "    hidden_sizes = []\n",
    "    for i in range(n_layers):\n",
    "        size = trial.suggest_int(f'hidden_size_{i}', 2, 64)\n",
    "        hidden_sizes.append(size)\n",
    "    \n",
    "    # 3. Instantiate a model with suggested hyperparameters\n",
    "    model = MLP(hidden_sizes)\n",
    "    \n",
    "    # 4. Cross-validate the suggested model\n",
    "    scores = cross_validate(model, X_full_train, Y_full_train[:, 0].reshape(-1, 1), strata_full_train, kf, pca=True)\n",
    "    return np.mean(scores)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fb462f36b249e772",
   "metadata": {},
   "source": [
    "The creation of this study object was placed here in markdown, to avoid running it unnecessarily.\n",
    "\n",
    "```python\n",
    "# 5. Create a study object and optimize the objective function.\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "```\n",
    "\n",
    "Each of the models in the following section are instantiated with a number of hidden neurons that came about through this process.\n",
    "\n",
    "# 5 - Multi-Target Regression Methods\n",
    "\n",
    "We will now consider three types of architectures for estimating the full GRFs.\n",
    "Each architecture has a variant with and a variant without PCA.\n",
    "Performance on the training and test set will be determined using this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80a81da11a5d68d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T14:04:58.676592Z",
     "start_time": "2024-05-20T14:04:58.651417Z"
    }
   },
   "source": [
    "def evaluate(model, X, Y):\n",
    "    def error(Y, Y_pred):\n",
    "        BODY_WEIGHT = 84 # kg\n",
    "        loss_function = MSELoss()\n",
    "        return np.sqrt(loss_function(Y, Y_pred).item()) * BODY_WEIGHT \n",
    "\n",
    "    # Make predictions\n",
    "    Y_pred = model(X)\n",
    "    \n",
    "    # Build dictionary of errors\n",
    "    errors = []\n",
    "    \n",
    "    # Overall error\n",
    "    errors.append(error(Y, Y_pred))   \n",
    "    \n",
    "    # Error per target variable\n",
    "    for i, label in enumerate(LABELS):\n",
    "        y = Y[:, i].reshape(-1, 1)\n",
    "        y_pred = Y_pred[:, i].reshape(-1, 1)\n",
    "        errors.append(error(y, y_pred))\n",
    "    \n",
    "    return errors"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3db4e2c2fad96e5f",
   "metadata": {},
   "source": [
    "We will plot them in a barchart with matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b866f608c15393c",
   "metadata": {},
   "source": [
    "def compare_errors(model1, model2):\n",
    "    targets = ['Total', 'Fx_l', 'Fy_l', 'Fz_l', 'Fx_r', 'Fy_r', 'Fz_r']\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12,3), sharey=True, sharex=True)\n",
    "    x_axis = np.arange(len(targets)) \n",
    "\n",
    "    # Compute In-Sample Error\n",
    "    errors1 = evaluate(model1, X_full_train, Y_full_train)\n",
    "    errors2 = evaluate(model2, X_pc_full_train, Y_full_train)\n",
    "\n",
    "    # Plot In-Sample Error\n",
    "    axs[0].set_title(\"In-Sample Error\") \n",
    "    axs[0].bar(x_axis - 0.2, errors1, 0.4, label = 'no PCA') \n",
    "    axs[0].bar(x_axis + 0.2, errors2, 0.4, label = 'PCA') \n",
    "    axs[0].set_xticks(x_axis, targets) \n",
    "    axs[0].set_xlabel(\"Target Variable\") \n",
    "    axs[0].set_ylabel(\"MSE (in kg)\") \n",
    "    axs[0].legend() \n",
    "\n",
    "    # Compute Out-Of-Sample Error\n",
    "    errors1 = evaluate(model1, X_test, Y_test)\n",
    "    errors2 = evaluate(model2, X_pc_test, Y_test)\n",
    "    \n",
    "    # Plot Out-Of-Sample Error\n",
    "    axs[1].set_title(\"Out-Of-Sample Error\") \n",
    "    axs[1].bar(x_axis - 0.2, errors1, 0.4, label = 'no PCA') \n",
    "    axs[1].bar(x_axis + 0.2, errors2, 0.4, label = 'PCA')\n",
    "    axs[1].set_xlabel(\"Target Variable\") \n",
    "    axs[1].yaxis.set_tick_params(labelleft=True)\n",
    "    \n",
    "    plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c003e2d59d5b966d",
   "metadata": {},
   "source": [
    "## 5.1 - Single-Target Method\n",
    "\n",
    "The most straight-forward way of estimating multiple target variables, is by dedicating a seperate MLP to each one.\n",
    "By doing so, we essentially treat the multi-target regression problem as six separate single-target regression problems.\n",
    "\n",
    "The `SingleTargetRegressor` implemented in the following code block acts as no more than a container holding a submodel for each GRF component.\n",
    "Each submodel is trained on the same input data, but on a different target label (see `train_`-method).\n",
    "From then on, they can be used to make predictions for their dedicated target (see `forward`-method).\n",
    "As such, the biggest shortcoming of this method immediately becomes apparent: it doesn't provide a means of exploiting the relationship between target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b971a7ea65f46976",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T12:21:58.378009Z",
     "start_time": "2024-05-20T12:21:58.337735Z"
    }
   },
   "source": [
    "class SingleTargetRegressor(nn.Module):\n",
    "    \"\"\"Implementation of the single-target method\"\"\"\n",
    "    \n",
    "    def __init__(self, sub_models):\n",
    "        super().__init__()\n",
    "        # Instantiate the submodels\n",
    "        self.sub_models = sub_models\n",
    "\n",
    "    def train_(self, X, Y):\n",
    "        # Train a dedicated MLP for each target label\n",
    "        self.sub_models['Fx_l'].train_(X, Y[:, 0].reshape(-1, 1))\n",
    "        self.sub_models['Fy_l'].train_(X, Y[:, 1].reshape(-1, 1))\n",
    "        self.sub_models['Fz_l'].train_(X, Y[:, 2].reshape(-1, 1))\n",
    "        self.sub_models['Fx_r'].train_(X, Y[:, 3].reshape(-1, 1))\n",
    "        self.sub_models['Fy_r'].train_(X, Y[:, 4].reshape(-1, 1))\n",
    "        self.sub_models['Fz_r'].train_(X, Y[:, 5].reshape(-1, 1))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # Make predictions for each component using the relevant MLP\n",
    "        Fx_l = self.sub_models['Fx_l'](X)\n",
    "        Fy_l = self.sub_models['Fy_l'](X)\n",
    "        Fz_l = self.sub_models['Fz_l'](X)\n",
    "        Fx_r = self.sub_models['Fx_r'](X)\n",
    "        Fy_r = self.sub_models['Fy_r'](X)\n",
    "        Fz_r = self.sub_models['Fz_r'](X)\n",
    "        \n",
    "        # Concatenate the predictions to a single output\n",
    "        Y = torch.cat([Fx_l, Fy_l, Fz_l,\n",
    "                       Fx_r, Fy_r, Fz_r], dim=1)\n",
    "        return Y"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4176014c6466aa3",
   "metadata": {},
   "source": [
    "The following submodels were the best ones considered during hyperparameter tuning.\n",
    "For each model, Optuna suggested one or two hidden layers with a maximum of 64 neurons per layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3a19b2ccf054579",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T14:31:41.328746Z",
     "start_time": "2024-05-20T14:31:16.712409Z"
    }
   },
   "source": [
    "##########################\n",
    "# BEST MODEL WITHOUT PCA #\n",
    "##########################\n",
    "best_submodels = {\n",
    "    'Fx_l': MLP([45]),     'Fy_l': MLP([58, 35]), 'Fz_l': MLP([59, 54]),\n",
    "    'Fx_r': MLP([21, 24]), 'Fy_r': MLP([60, 32]), 'Fz_r': MLP([51, 46])\n",
    "}\n",
    "single_target_regressor = SingleTargetRegressor(best_submodels)\n",
    "single_target_regressor.train_(X_full_train, Y_full_train)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "991f0f330af12e81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T14:32:01.935167Z",
     "start_time": "2024-05-20T14:31:41.328746Z"
    }
   },
   "source": [
    "##########################\n",
    "# BEST MODEL WITH PCA #\n",
    "##########################\n",
    "best_submodels_pca = {\n",
    "    'Fx_l': MLP([59, 24]), 'Fy_l': MLP([54, 32]), 'Fz_l': MLP([61]),\n",
    "    'Fx_r': MLP([59, 23]), 'Fy_r': MLP([62, 57]), 'Fz_r': MLP([58])\n",
    "}\n",
    "\n",
    "single_target_regressor_pca = SingleTargetRegressor(best_submodels_pca)\n",
    "single_target_regressor_pca.train_(X_pc_full_train, Y_full_train)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db289c7d523db014",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T14:32:02.955061Z",
     "start_time": "2024-05-20T14:32:01.937202Z"
    }
   },
   "source": [
    "compare_errors(single_target_regressor, single_target_regressor_pca)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "69e5becf5a5bf466",
   "metadata": {},
   "source": [
    "As we can see, the PCA-approach results in a lower MSE, both in-sample and out-of-sample.\n",
    "The only exception is $F_y$ of the left foot.\n",
    "The $z$-component of force gives the highest errors in both cases.\n",
    "However, we should note that this is to be expected, given that this is the biggest component of the tree.\n",
    "Relative to the magnitude of its force, the errors for $F_y$ are actually the highest.\n",
    "\n",
    "## 5.2 - Regressor Stacking\n",
    "\n",
    "Regressor stacking is an alternative approach, that will allow the predictions for components to learn from each other.\n",
    "To do so, we'll start again by seperately training single-target **base regressors** for each component.\n",
    "Their outputs ($\\hat{y}_1^*$ to $\\hat{y}_6^*$) serve as initial predictions that will be improved upon with the use of six **meta-models**. \n",
    "Each meta-model will be trained on the original features, as well as the predictions made by the base regressors.\n",
    "As such, we'll dedicate one meta-model to each target variable.\n",
    "The idea is that $\\hat{y}_1^*$ to $\\hat{y}_6^*$ can help the meta-models to correct the predictions of the base regressors.\n",
    "In general, all estimations will be informed by the initial predictions for all target variables.\n",
    "The following image illustrates this idea with a diagram of a simpler scenario with 2 target variables:\n",
    "\n",
    "![test](images/regressor-stacking.png)\n",
    "\n",
    "\n",
    "To see how this is useful, consider the following example.\n",
    "Let $y_1$ and $y_2$ be the vertical GRF of the left and right foot, respectively.\n",
    "Suppose that $\\mathrm{Regressor}_1$ makes an estimation of $\\hat{y}_1 = 0$, meaning that the left foot is off the ground.\n",
    "If this prediction is correct, this implies that (in a normal walking trial) the right foot is bearing all the weight of the subject.\n",
    "Through $\\hat{y}_1$, $\\mathrm{Regressor}_2$ can now take this into account when making a prediction for $y_2$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "637fcd309a13b98c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T13:29:51.254481Z",
     "start_time": "2024-05-20T13:29:51.232483Z"
    }
   },
   "source": [
    "class StackedRegressor(nn.Module):\n",
    "    \"\"\"Implementation of the multi-target regressor, using the single-target method\"\"\"\n",
    "    \n",
    "    def __init__(self, base_models, meta_models):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Instantiate MLP for each component\n",
    "        self.base_regressor = SingleTargetRegressor(base_models)    \n",
    "        self.meta_models = meta_models\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Make intitial predictions with the base models\n",
    "        Y_base = self.base_regressor(X)   \n",
    "    \n",
    "        # Extend X with these initial predictions\n",
    "        X_meta = torch.cat([X, Y_base], dim=1)\n",
    "    \n",
    "        # Make final predictions for each component using the relevant meta model\n",
    "        Fx_l = self.meta_models['Fx_l'](X_meta)\n",
    "        Fy_l = self.meta_models['Fy_l'](X_meta)\n",
    "        Fz_l = self.meta_models['Fz_l'](X_meta)\n",
    "        Fx_r = self.meta_models['Fx_r'](X_meta)\n",
    "        Fy_r = self.meta_models['Fy_r'](X_meta)\n",
    "        Fz_r = self.meta_models['Fz_r'](X_meta)\n",
    "        \n",
    "        # Concatenate the final predictions to a single output\n",
    "        Y_meta = torch.cat([Fx_l, Fy_l, Fz_l,\n",
    "                            Fx_r, Fy_r, Fz_r], dim=1)\n",
    "        return Y_meta\n",
    "\n",
    "    def train_(self, X, Y):\n",
    "        # Divide the training set into a half for the base models and a half for the meta models\n",
    "        (X_base, X_meta,\n",
    "         Y_base, Y_meta) = train_test_split(X, Y, test_size=0.5, random_state=42)\n",
    "        \n",
    "        # Train the base models\n",
    "        self.base_regressor.train_(X_base, Y_base)\n",
    "\n",
    "        # Add initial predictions to meta input set\n",
    "        Y_base_pred = self.base_regressor(X_meta)\n",
    "        X_meta_extended = torch.cat([X_meta, Y_base_pred], dim=1).detach()\n",
    "        \n",
    "        # Train each MLP on the relevant target label\n",
    "        self.meta_models['Fx_l'].train_(X_meta_extended, Y_meta[:, 0].reshape(-1, 1))\n",
    "        self.meta_models['Fy_l'].train_(X_meta_extended, Y_meta[:, 1].reshape(-1, 1))\n",
    "        self.meta_models['Fz_l'].train_(X_meta_extended, Y_meta[:, 2].reshape(-1, 1))\n",
    "        self.meta_models['Fx_r'].train_(X_meta_extended, Y_meta[:, 3].reshape(-1, 1))\n",
    "        self.meta_models['Fy_r'].train_(X_meta_extended, Y_meta[:, 4].reshape(-1, 1))\n",
    "        self.meta_models['Fz_r'].train_(X_meta_extended, Y_meta[:, 5].reshape(-1, 1))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "48ce518a18c66734",
   "metadata": {},
   "source": [
    "To keep the overall complexity of the model similar to that of the `SingleTargetRegressor`, we limited all base and meta regressors to one hidden layer with a maximum of 64 neurons.\n",
    "Before training, an arbitrary split was in the training data.\n",
    "One half was used for training the base regressors, while the other half was used for training the meta models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c91d40d90edc46d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T13:30:16.941909Z",
     "start_time": "2024-05-20T13:29:58.634532Z"
    }
   },
   "source": [
    "##########################\n",
    "# BEST MODEL WITHOUT PCA #\n",
    "##########################\n",
    "best_base_models = {\n",
    "    'Fx_l': MLP([50]), 'Fy_l': MLP([51]), 'Fz_l': MLP([61]),\n",
    "    'Fx_r': MLP([40]), 'Fy_r': MLP([38]), 'Fz_r': MLP([53])\n",
    "}\n",
    "\n",
    "best_meta_models = {\n",
    "    'Fx_l': MLP([33]), 'Fy_l': MLP([31]), 'Fz_l': MLP([51]),\n",
    "    'Fx_r': MLP([58]), 'Fy_r': MLP([46]), 'Fz_r': MLP([59])\n",
    "}\n",
    "\n",
    "stacked_regressor = StackedRegressor(best_base_models, best_meta_models)\n",
    "stacked_regressor.train_(X_full_train, Y_full_train)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ada0d4512b6b380",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T13:31:44.217539Z",
     "start_time": "2024-05-20T13:31:27.391848Z"
    }
   },
   "source": [
    "#######################\n",
    "# BEST MODEL WITH PCA #\n",
    "#######################\n",
    "best_base_models_pca = {\n",
    "    'Fx_l': MLP([62]), 'Fy_l': MLP([54]), 'Fz_l': MLP([64]),\n",
    "    'Fx_r': MLP([64]), 'Fy_r': MLP([61]), 'Fz_r': MLP([64])\n",
    "}\n",
    "\n",
    "best_meta_models_pca = {\n",
    "    'Fx_l': MLP([43]), 'Fy_l': MLP([64]), 'Fz_l': MLP([59]),\n",
    "    'Fx_r': MLP([50]), 'Fy_r': MLP([60]), 'Fz_r': MLP([63])\n",
    "}\n",
    "\n",
    "stacked_regressor_pca = StackedRegressor(best_base_models_pca, best_meta_models_pca)\n",
    "stacked_regressor_pca.train_(X_pc_full_train, Y_full_train)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44f40e26ff378a53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T14:32:04.157779Z",
     "start_time": "2024-05-20T14:32:02.955061Z"
    }
   },
   "source": [
    "compare_errors(stacked_regressor, stacked_regressor_pca)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "180070a94f0b5ec9",
   "metadata": {},
   "source": [
    "Again, PCA seems to have a positive influence on the errors.\n",
    "Unfortunately, we notice that the goal of the stacking method has failed: errors didn't decrease compared to the previous method.\n",
    "\n",
    "## 5.3 - Multi-Task Learning With Hard Parameter Sharing\n",
    "\n",
    "Despite its complicated name, the final method is actually the simplest with regards to model architecture.\n",
    "We can simply dedicate an output neuron to each of the target variables, leading to an output layer of 6 neurons.\n",
    "By adapting the cost function to include predictions and targets of all 6 outputs, we can train them simultaneously.\n",
    "This simultaneous learning is what's referred to as \"multi-task learning\".\n",
    "It's not always clear when this is to the benefit and when this is to the detriment of the final predictions.\n",
    "As a rule of thumb, it doesn't hurt if output variables are related to each other in some way, which we can assume is the case in our conext.\n",
    "The \"hard parameter sharing\" refers to the fact that all output neurons are preceded by shared layers only.\n",
    "Alternatively, one could construct an ANN where each output has its own \"tail\" of hidden layers that are not connected to the other outputs.\n",
    "This would be referred to as \"soft parameter sharing\"\n",
    "\n",
    "Originally, we tuned the hyperparameters of this model in the same way of the previous ones.\n",
    "This gave us the following MLPs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46fce7943ed57df9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T14:41:12.758161Z",
     "start_time": "2024-05-20T14:41:06.986797Z"
    }
   },
   "source": [
    "multi_task_regressor = MLP([64])\n",
    "multi_task_regressor.train_(X_full_train, Y_full_train)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1b76b7d692a0e44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T14:41:31.320931Z",
     "start_time": "2024-05-20T14:41:26.024443Z"
    }
   },
   "source": [
    "multi_task_regressor_pca = MLP([63, 47])\n",
    "multi_task_regressor_pca.train_(X_pc_full_train, Y_full_train)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "53d9a2c8-0f56-4ef6-af36-4621f122de56",
   "metadata": {},
   "source": [
    "Once again, errors were higher than with the previous approach.\n",
    "However, we figured that it might be unfair to limit this MLP to the same number of hidden neurons.\n",
    "Previous methods resulted in a compound model.\n",
    "Even though each of their submodels was limited to hidden layers of 64 neurons, together they offer a lot more compute.\n",
    "We therefore rerun the hyperparameter tuning, but hidden neurons could now be suggested up until $6 \\times 64$ neurons.\n",
    "The results were the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca5ee45b-4dde-4273-ab8a-574005a2f414",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T14:41:12.758161Z",
     "start_time": "2024-05-20T14:41:06.986797Z"
    }
   },
   "source": [
    "multi_task_regressor = MLP([318, 77])\n",
    "multi_task_regressor.train_(X_full_train, Y_full_train)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0240e3ad-c3a5-4ed6-ae77-5e9aa8b2ba4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T14:41:31.320931Z",
     "start_time": "2024-05-20T14:41:26.024443Z"
    }
   },
   "source": [
    "multi_task_regressor_pca = MLP([209])\n",
    "multi_task_regressor_pca.train_(X_pc_full_train, Y_full_train)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ce6e24c6afb4d64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T14:41:32.223254Z",
     "start_time": "2024-05-20T14:41:31.320931Z"
    }
   },
   "source": [
    "compare_errors(multi_task_regressor, multi_task_regressor_pca)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dbd6717c9d2f1555",
   "metadata": {},
   "source": [
    "There's a noticeable difference between the PCA and non-PCA variant, even more so than with the previous methods.\n",
    "This time, the errors of the PCA model are comparable to those of the single-target method.\n",
    "During our runs, they are even slightly lower.\n",
    "\n",
    "# 6 - Conclusion\n",
    "\n",
    "To conclude, we will make a final comparison between all of the previous approaches.\n",
    "\n",
    "Let's start by addressing the first subquestion: **\"What's the impact of PCA on the network's in- and out-of-sample error?\"**\n",
    "\n",
    "For each of the three types of architectures, the PCA-version resulted in lower errors, both in-sample and out-of-sample.\n",
    "This might be because of several reasons.\n",
    "\n",
    "- First, reducing the dimensionality of the input data makes the learning problem simpler in the sense that there are less input features.\n",
    "  Since these preserve $99\\%$ of the variance, we shouldn't lose any useful information in the process.\n",
    "- Second, selecting the first $n$ principal components acts as a form of feature selection.\n",
    "  The downside for us humans is that these new features are harder to interpret.\n",
    "  An ANN on the other hand, works through abstract feature learning anyway.\n",
    "  The linear transformation that is made by the PCA should not \"bother\" the MLP, because it works with linear combinations by design.\n",
    "  In a sense, we give the network a \"head start\" by deliberately creating a highly informative linear combination of the variables in the original input data.\n",
    "- Third, it's possible that PCA improved the results incidentally through the scaling or noise reduction.\n",
    "  In that case it's not the inherent nature of principal components itself that reduced errors.\n",
    "  This means other methods of preprocessing might give even better results.\n",
    "  For example, the reason that PCA benefits this specific learning problem might be due to the multiple timestamps included in X.\n",
    "  Other choices could have been made regarding this preprocessing.\n",
    "\n",
    "Of course this doesn't mean that performing PCA is advisable in all deep learning contexts.\n",
    "ANNs are known as an instrument for end-to-end learning.\n",
    "The conclusions made in this notebook should not be overinterpreted onto other architectures and learning problems.\n",
    "For instance, experimenting with other datasets, other hyperparameters or other implementations of gradient descent might lead to different conclusions.\n",
    "\n",
    "\n",
    "Now, regarding the second subquestion: **\"How do different multi-target regression methods compare in terms of in- and out-of-sample error?\"**\n",
    "\n",
    "We considered the single-target method, stacking and multi-task learning with hard parameter sharing.\n",
    "For the complexities considered, the PCA version of the single-target method and the multi-task method gave comparable errors, both in-sample and out-of-sample.\n",
    "The multi-task model even seems to get a slight edge in the runs we considered.\n",
    "\n",
    "The stacking method gave the highest errors and did not succeed in its effort of exploiting the relationships between target variables.\n",
    "This might be due to the fact that this relationship is very complex.\n",
    "Indeed, datapoints have been collected in a wide variety of gait trials. \n",
    "Although one can imagine simple relationships between GRFs in an isolated trial (for example with the example of forward walking earlier on), it becomes way harder to do this when all trials are piled together.\n",
    "To illustrate this, the following cell of code plots the correlation between target variables.\n",
    "On the left, only the data of a single \"forward walking\" is included.\n",
    "On the left we potted correlation for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2079e7d-05d0-46ac-962d-ef5f6124106a",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Correlation plot for forward walking\n",
    "corr1 = gait_cycles[gait_cycles['trial'] == 'FW_walking_01.csv'].iloc[:, :6].corr()\n",
    "cmap = sns.diverging_palette(260, 20, s=75, l=40, n=5, center=\"light\", as_cmap=True)\n",
    "sns.heatmap(corr1, cmap=cmap, center=0, annot=True, fmt='.2f', square=True, robust=True, cbar=False, ax=axes[0])\n",
    "axes[0].set_title('Forward Walking')\n",
    "\n",
    "# Correlation plot for all trials\n",
    "corr2 = gait_cycles.iloc[:, :6].corr()\n",
    "sns.heatmap(corr2, cmap=cmap, center=0, annot=True, fmt='.2f', square=True, robust=True, cbar=False, ax=axes[1])\n",
    "axes[1].set_title('All Trials')\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a897e97c-9f48-4dd6-8bc5-eccabcad9a6c",
   "metadata": {},
   "source": [
    "However, keep in mind that ANNs are capable of learning non-linear relationships as well.\n",
    "For that reason, these correlation plots are by no means an indisputable justification for the lack of performance gains of stacking.\n",
    "\n",
    "We conclude that the single-target method and multi-task method with hard parameter sharing perform similarly with regard to in- and out-of-sample error.\n",
    "Because there's only a minor difference, it's hard to make conclusions on whether the latter succeeded at finding a relationship between the output variables.\n",
    "The single target method is more approachable for ML engineers with limited to no experience in multi-output regression.\n",
    "However, once you get familiar with the abstraction of training multiple output variables at once, the architecture of the multi-output method becomes easier to implement.\n",
    "For data scientists who like an incremental approach, we'd suggest starting with a single target regressor for one of the target variables.\n",
    "Once a decent model for that component has been found, one has a better intuition of the complexity that might be necessary for the multi-output regressor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
